{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPm8WD5LZwaCftSGwgS1ooE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Duchai263/QAbot/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installation"
      ],
      "metadata": {
        "id": "f70L3VUn1R90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojdG5qFfD-Xw",
        "outputId": "be9e38db-e023-4f62-a3e8-6b8e39e8248a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/2.5 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.4/151.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain-huggingface text-generation transformers google-search-results numexpr langchainhub sentencepiece jinja2 bitsandbytes accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2vXD47MJuOW",
        "outputId": "d7cd40d5-6b98-4001-d0f6-cd3a43f246c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-huggingface"
      ],
      "metadata": {
        "id": "RY0YUZaSKQAI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-community"
      ],
      "metadata": {
        "id": "BEC-J9isKWXh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-community faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUyySc9TK59Q",
        "outputId": "9d3f9fdb-d5d4-4166-dbae-92e547c9870e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6NnLhBFLnl8",
        "outputId": "05d7968e-6ac1-486f-c075-1e9560ce38cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval"
      ],
      "metadata": {
        "id": "z25MnrnAJxu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name= \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZV2S3FoJxUB",
        "outputId": "befb48ed-0144-47e1-ea70-45166f1245b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name TinyLlama/TinyLlama-1.1B-Chat-v1.0. Creating a new one with mean pooling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"Init vector\")))\n",
        "\n",
        "vector_store = FAISS(\n",
        "    embedding_function=embeddings,\n",
        "    index=index,\n",
        "    docstore=InMemoryDocstore(),\n",
        "    index_to_docstore_id={},\n",
        ")"
      ],
      "metadata": {
        "id": "-N8J7o7oKuzL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "def add_document(doc: Document):\n",
        "  vector_store.add_documents([doc])\n",
        "\n",
        "def get_document(query: str):\n",
        "  return vector_store.similarity_search(query, k=1)"
      ],
      "metadata": {
        "id": "npsguUD1pjWT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test"
      ],
      "metadata": {
        "id": "4Vhfie2wD44P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"policies.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  context = f.read()\n",
        "  doc = Document(page_content=context, metadata={\"source\": \"policies.txt\"})\n",
        "  add_document(doc)"
      ],
      "metadata": {
        "id": "IlGFuHWq5YYy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "docs_path = [\"policies1.txt\",\"policies2.txt\",\"policies3.txt\"]\n",
        "documents = []\n",
        "for doc in docs_path:\n",
        "  with open(doc,\"r\",encoding=\"utf-8\") as f:\n",
        "    documents.append(Document(page_content=f.read(), metadata={\"source\": doc}))\n",
        "\n",
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7dTPsYMNnPQ",
        "outputId": "5f8fd8da-b59a-420f-d192-d876ebc53983"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fb7fed5f-bf96-4c8c-9ee1-1de7ad63b191',\n",
              " '5bcea8df-882b-4bf3-943a-a09e66c76494',\n",
              " '1f212aaa-81cb-42cb-a482-58a9218228ad']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Can I come late at 11 AM?\"\n",
        "\n",
        "results = vector_store.similarity_search(\n",
        "    question,\n",
        "    k=1\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw5hNq90O6ON",
        "outputId": "de465e66-2854-416b-ea94-321806ecc143"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Working Time\n",
            "Working Time\n",
            "From 9.00 AM- 6.00 PM, Monday – Friday ( or 8.00 AM- 5.00 PM, as long as you can ensure 8 hours of work per day)\n",
            "Lunch break for 01 hour (from 12:00 PM to 1:00 PM)\n",
            "Possibility to flex before 10.00 AM and after 4.00 PM. The team and engineering coach must be informed if flex time is taken between 10.00 AM to 4.00 PM\n",
            "You can go to the office on  the weekend for work but you need to inform your team and Eng. Coach on Friday as the latest before the weekend.   Employees who would like to work at the office on Sunday have to inform Mrs. Linh before 4.00 PM on Friday to register with the BOM of the PJICO Building.\n",
            " [{'source': 'policies1.txt'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create model"
      ],
      "metadata": {
        "id": "FKDBBxh31XVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "# model_id = \"TinyLlama/TinyLlama_v1.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer,\n",
        "                do_sample=False,\n",
        "                temperature=None,\n",
        "                top_p=None,\n",
        "                repetition_penalty=1.2,\n",
        "                # add_generation_prompt=False\n",
        "                max_new_tokens=200,\n",
        "                )\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "template = \"\"\"You are a business assistant. Provide a answer to the employee's question with given context.\n",
        "context: {context}\n",
        "\n",
        "question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "chain = prompt | llm.bind(skip_prompt=True)\n",
        "\n",
        "\n",
        "def get_answer(question: str):\n",
        "  results = vector_store.similarity_search(question,k=1)\n",
        "\n",
        "  context = results[0].page_content\n",
        "  soucrce = results[0].metadata[\"source\"]\n",
        "\n",
        "  answer = chain.invoke({\"context\": context, \"question\": question})\n",
        "  return {\"answer\": answer, \"source\": soucrce}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqM-4O1TH944",
        "outputId": "33dbb9b0-9a91-4907-f55b-a560ec939757"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test"
      ],
      "metadata": {
        "id": "wDZHy_7YCy69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is the Working Time\"\n",
        "\n",
        "print(f\"Question: {question} \")\n",
        "print(f\"\\nModel name: {model_id}\")\n",
        "\n",
        "response = get_answer(question)\n",
        "answer = response[\"answer\"]\n",
        "source = response[\"source\"]\n",
        "\n",
        "print(f\"Answer: {answer}\")\n",
        "print(f\"Source: {source}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojy5V8AJ3ijZ",
        "outputId": "5f4bb023-e8f4-4cdc-d9ee-e7f114162fac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: what is the Working Time \n",
            "\n",
            "Model name: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "Answer: Working time means the number of hours an individual works or spends on work activities. It includes all types of work such as office work, manual labor, and other non-office related tasks. The term ‘working time’ refers to the total amount of time that an employee has been assigned by their employer to perform specific duties within a particular organization. In our example, the working time is 8 hours per day, which can be divided into two parts – morning and afternoon shifts.\n",
            "Source: policies2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"policies1.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  context = f.read()\n",
        "print(context)"
      ],
      "metadata": {
        "id": "RDJ7b3ebHZCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb991fd-ae37-4ea2-8338-891f75276daa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working Time\n",
            "Working Time\n",
            "From 9.00 AM- 6.00 PM, Monday – Friday ( or 8.00 AM- 5.00 PM, as long as you can ensure 8 hours of work per day)\n",
            "Lunch break for 01 hour (from 12:00 PM to 1:00 PM)\n",
            "Possibility to flex before 10.00 AM and after 4.00 PM. The team and engineering coach must be informed if flex time is taken between 10.00 AM to 4.00 PM\n",
            "You can go to the office on  the weekend for work but you need to inform your team and Eng. Coach on Friday as the latest before the weekend.   Employees who would like to work at the office on Sunday have to inform Mrs. Linh before 4.00 PM on Friday to register with the BOM of the PJICO Building.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dXsH17pHAAH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "question = \"Can I come late at 11 AM?\"\n",
        "\n",
        "response = chain.invoke({\"context\": context, \"question\": question})\n",
        "\n",
        "print(f\"Question: {question} \")\n",
        "print(f\"\\nModel name: {model_id}\")\n",
        "print(f\"Answer: {response}\")"
      ],
      "metadata": {
        "id": "vWTIc4J7OScd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62fd84c7-9f5b-4b8f-8073-5265673ccd64"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Can I come late at 11 AM? \n",
            "\n",
            "Model name: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "Answer: Yes, you may come late at 11 AM provided that you notify your supervisor in advance via email or phone call. If you arrive later than this time slot without prior notice, it will result in an unscheduled absence from work.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flask app - ngrok"
      ],
      "metadata": {
        "id": "GJxwWOxG-_ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "import threading\n",
        "\n",
        "from flask import Flask, request, jsonify, render_template\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "conf.get_default().auth_token = getpass.getpass()\n",
        "\n",
        "app = Flask(__name__)\n",
        "port = \"5000\"\n",
        "\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
        "\n",
        "# Update any base URLs to use the public ngrok URL\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "\n",
        "# ... Update inbound traffic via APIs to use the public-facing ngrok URL\n",
        "\n",
        "\n",
        "def get_ai_response(user_input: str) -> str:\n",
        "    response = get_answer(user_input)\n",
        "    answer = response[\"answer\"]\n",
        "    source = response[\"source\"]\n",
        "\n",
        "    return f\"{answer} \\n Source: {source}\"\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    user_input = request.json.get('message')\n",
        "    response = get_ai_response(user_input)\n",
        "    return jsonify({'response': response})\n",
        "\n",
        "@app.route('/upload', methods=['POST'])\n",
        "def upload_file():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({'error': 'No file part'}), 400\n",
        "    file = request.files['file']\n",
        "    if file.filename == '' or not file.filename.lower().endswith('.txt'):\n",
        "        return jsonify({'error': 'Invalid file format. Only .txt files are allowed.'}), 400\n",
        "\n",
        "\n",
        "    file_content = file.read().decode('utf-8')\n",
        "    file_name = file.filename\n",
        "\n",
        "    doc = Document(page_content=file_content, metadata={\"source\": file_name})\n",
        "    add_document(doc)\n",
        "\n",
        "    return jsonify({'message': 'File content read successfully.', 'content': file_content, 'filename': file_name})\n",
        "# Start the Flask server in a new thread\n",
        "threading.Thread(target=app.run, kwargs={\"use_reloader\": False}).start()\n",
        "# app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw-SSao7_Dj0",
        "outputId": "1f401253-2939-4631-d304-48511e29eb18"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\n",
            "··········\n",
            " * ngrok tunnel \"https://5a7e-34-124-151-240.ngrok-free.app\" -> \"http://127.0.0.1:5000\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -eaf | grep 5000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzCUSoCoOb_a",
        "outputId": "19c7871f-1afb-4897-b192-8b763eb0ceb8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        4766    1126  0 06:37 ?        00:00:00 /bin/bash -c ps -eaf | grep 5000\n",
            "root        4768    4766  0 06:37 ?        00:00:00 grep 5000\n"
          ]
        }
      ]
    }
  ]
}